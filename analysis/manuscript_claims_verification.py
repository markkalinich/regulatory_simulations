#!/usr/bin/env python3
"""
Manuscript Claims Verification - Verify paper claims against pipeline output

Reads expected claims from config/manuscript_claims.json and compares against
actual data generated by the regulatory simulation paper pipeline.

Usage:
    python analysis/manuscript_claims_verification.py --paper-run-dir results/REGULATORY_SIMULATION_PAPER/[timestamp]
"""

import pandas as pd
import numpy as np
from pathlib import Path
import argparse
import json

ROOT = Path(__file__).parent.parent
RESULTS_DIR = ROOT / "results"
CLAIMS_FILE = ROOT / "config" / "manuscript_claims.json"


def load_claims():
    """Load manuscript claims from JSON file"""
    with open(CLAIMS_FILE) as f:
        return json.load(f)


def verify_section1_gemini_data(output: list, claims: dict) -> list:
    """Verify Section 1: Gemini-Generated Synthetic Data claims"""
    
    output.append("## Section 1: Gemini-Generated Synthetic Data (Figure 3)\n")
    
    section_claims = claims['section_1_gemini_data']['claims']
    review_stats = RESULTS_DIR / "review_statistics"
    
    # Load review statistics
    si_stats = pd.read_csv(review_stats / 'si_review_statistics.csv')
    tr_stats = pd.read_csv(review_stats / 'therapy_request_review_statistics.csv')
    te_stats = pd.read_csv(review_stats / 'therapy_engagement_review_statistics.csv')
    chi_sq = pd.read_csv(review_stats / 'chi_squared_tests.csv')
    
    # Claim 1: Non-SI statements retained
    claim = section_claims['non_si_retained']
    non_si_row = si_stats[si_stats['Category'] == 'SUBTOTAL: Non-SI'].iloc[0]
    actual_num = non_si_row['Approved (No Changes)']
    actual_denom = non_si_row['Generated']
    passed = actual_num == claim['numerator'] and actual_denom == claim['denominator']
    output.append(f"{'✅ PASSED' if passed else '❌ FAILED'}")
    output.append(f"Claim: Non-SI statements retained {claim['value']}")
    output.append(f"Data: {actual_num}/{actual_denom}")
    output.append("")
    
    # Claim 2: SI content retained
    claim = section_claims['si_retained']
    si_row = si_stats[si_stats['Category'] == 'SUBTOTAL: SI'].iloc[0]
    actual_num = si_row['Approved (No Changes)']
    actual_denom = si_row['Generated']
    passed = actual_num == claim['numerator'] and actual_denom == claim['denominator']
    output.append(f"{'✅ PASSED' if passed else '❌ FAILED'}")
    output.append(f"Claim: SI content retained {claim['value']}")
    output.append(f"Data: {actual_num}/{actual_denom}")
    output.append("")
    
    # Claim 3: SI vs non-SI p < 0.001
    claim = section_claims['si_retained']
    si_chi = chi_sq[chi_sq['Comparison'] == 'Non-SI vs SI'].iloc[0]
    p_value = si_chi['P_Value']
    passed = p_value < claim['p_value_threshold']
    output.append(f"{'✅ PASSED' if passed else '❌ FAILED'}")
    output.append(f"Claim: SI vs Non-SI comparison p < {claim['p_value_threshold']}")
    output.append(f"Data: p = {p_value:.2e} (χ² = {si_chi['Chi_Squared']:.2f})")
    output.append("")
    
    # Claim 4: Non-therapeutic (TR) retained
    claim = section_claims['non_therapeutic_tr_retained']
    tr_decl = tr_stats[tr_stats['Category'] == 'SUBTOTAL: Declarative Statements'].iloc[0]
    tr_nonther = tr_stats[tr_stats['Category'] == 'SUBTOTAL: Non-Therapeutic Questions'].iloc[0]
    actual_num = tr_decl['Approved (No Changes)'] + tr_nonther['Approved (No Changes)']
    actual_denom = tr_decl['Generated'] + tr_nonther['Generated']
    passed = actual_num == claim['numerator'] and actual_denom == claim['denominator']
    output.append(f"{'✅ PASSED' if passed else '❌ FAILED'}")
    output.append(f"Claim: Non-therapeutic statements retained {claim['value']}")
    output.append(f"Data: {actual_num}/{actual_denom}")
    output.append("")
    
    # Claim 5: Explicit therapy requests retained
    claim = section_claims['explicit_therapy_requests_retained']
    tr_req = tr_stats[tr_stats['Category'] == 'SUBTOTAL: Explicit Therapy Requests'].iloc[0]
    actual_num = tr_req['Approved (No Changes)']
    actual_denom = tr_req['Generated']
    passed = actual_num == claim['numerator'] and actual_denom == claim['denominator']
    output.append(f"{'✅ PASSED' if passed else '❌ FAILED'}")
    output.append(f"Claim: Explicit therapy requests retained {claim['value']}")
    output.append(f"Data: {actual_num}/{actual_denom}")
    output.append("")
    
    # Claim 6: TR request vs non-request p < 0.001
    claim = section_claims['explicit_therapy_requests_retained']
    tr_chi = chi_sq[chi_sq['Comparison'] == 'No Therapy Request vs Therapy Request'].iloc[0]
    p_value = tr_chi['P_Value']
    passed = p_value < claim['p_value_threshold']
    output.append(f"{'✅ PASSED' if passed else '❌ FAILED'}")
    output.append(f"Claim: TR request vs non-request comparison p < {claim['p_value_threshold']}")
    output.append(f"Data: p = {p_value:.2e} (χ² = {tr_chi['Chi_Squared']:.2f})")
    output.append("")
    
    # Claim 7: TE "nearly all" non-therapeutic and ambiguous retained
    claim = section_claims['te_non_therapeutic_ambiguous']
    te_nonther = te_stats[te_stats['Category'] == 'SUBTOTAL: Non-Therapeutic Conversations'].iloc[0]
    te_amb = te_stats[te_stats['Category'] == 'SUBTOTAL: Ambiguous Engagement'].iloc[0]
    combined_approved = te_nonther['Approved (No Changes)'] + te_amb['Approved (No Changes)']
    combined_total = te_nonther['Generated'] + te_amb['Generated']
    combined_pct = combined_approved / combined_total * 100
    passed = combined_pct >= claim['threshold_pct']
    output.append(f"{'✅ PASSED' if passed else '❌ FAILED'}")
    output.append(f"Claim: {claim['qualifier'].capitalize()} non-therapeutic and ambiguous retained (>{claim['threshold_pct']}%)")
    output.append(f"Data: {combined_approved}/{combined_total} = {combined_pct:.1f}%")
    output.append("")
    
    # Claim 8: TE "fewer than half" therapeutic without modification
    claim = section_claims['te_therapeutic_without_modification']
    te_ther = te_stats[te_stats['Category'] == 'SUBTOTAL: Therapeutic Conversation'].iloc[0]
    te_ther_no_changes = te_ther['Approved (No Changes)']
    te_ther_total = te_ther['Generated']
    te_ther_pct = te_ther_no_changes / te_ther_total * 100
    passed = te_ther_pct < claim['threshold_pct']
    output.append(f"{'✅ PASSED' if passed else '❌ FAILED'}")
    output.append(f"Claim: {claim['qualifier'].capitalize()} therapeutic exchanges met criteria (<{claim['threshold_pct']}%)")
    output.append(f"Data: {te_ther_no_changes}/{te_ther_total} = {te_ther_pct:.1f}%")
    output.append("")
    
    # Claim 9: TE therapeutic vs non-therapeutic p < 0.001
    claim = section_claims['te_p_value']
    te_chi = chi_sq[chi_sq['Comparison'] == 'Non-Therapeutic Interaction vs Therapeutic Conversation'].iloc[0]
    p_value = te_chi['P_Value']
    passed = p_value < claim['threshold']
    output.append(f"{'✅ PASSED' if passed else '❌ FAILED'}")
    output.append(f"Claim: TE therapeutic vs non-therapeutic comparison p < {claim['threshold']}")
    output.append(f"Data: p = {p_value:.2e} (χ² = {te_chi['Chi_Squared']:.2f})")
    output.append("")
    
    # Claim 10: CBT/DBT-based dialogues largely accurate
    claim = section_claims['cbt_dbt_accuracy']
    cbt_cog = te_stats[te_stats['Category'] == 'Therapeutic: Cognitive Technique'].iloc[0]
    cbt_skill = te_stats[te_stats['Category'] == 'Therapeutic: CBT/DBT Skill'].iloc[0]
    cbt_approved = cbt_cog['Approved (No Changes)'] + cbt_skill['Approved (No Changes)']
    cbt_total = cbt_cog['Generated'] + cbt_skill['Generated']
    cbt_pct = cbt_approved / cbt_total * 100
    passed = cbt_pct >= claim['threshold_pct']
    output.append(f"{'✅ PASSED' if passed else '❌ FAILED'}")
    output.append(f"Claim: CBT/DBT-based dialogues {claim['qualifier']} (>{claim['threshold_pct']}%)")
    output.append(f"Data: {cbt_approved}/{cbt_total} = {cbt_pct:.1f}% approved without changes")
    output.append("")
    
    # Claim 11: Psychodynamic/diagnosis/medication frequently broke frame
    claim = section_claims['psychodynamic_diagnosis_medication']
    psych = te_stats[te_stats['Category'] == 'Therapeutic: Psychodynamic'].iloc[0]
    diag = te_stats[te_stats['Category'] == 'Therapeutic: Diagnosis'].iloc[0]
    med = te_stats[te_stats['Category'] == 'Therapeutic: Med Recommendation'].iloc[0]
    
    psych_pct = psych['Approved (No Changes)'] / psych['Generated'] * 100
    diag_pct = diag['Approved (No Changes)'] / diag['Generated'] * 100
    med_pct = med['Approved (No Changes)'] / med['Generated'] * 100
    
    passed = psych_pct < claim['threshold_pct'] and diag_pct < claim['threshold_pct'] and med_pct < claim['threshold_pct']
    output.append(f"{'✅ PASSED' if passed else '❌ FAILED'}")
    output.append(f"Claim: Psychodynamic/diagnosis/medication {claim['qualifier']} (<{claim['threshold_pct']}% unchanged)")
    output.append(f"Data: Psychodynamic: {psych_pct:.1f}%, Diagnosis: {diag_pct:.1f}%, Medication: {med_pct:.1f}%")
    output.append("")
    
    return output


def verify_section2_difficult_statements(output: list, claims: dict) -> list:
    """Verify Section 2: Difficult statements missed by >50% of models"""
    
    output.append("## Section 2: Systematic shortcomings across models\n")
    
    section_claims = claims['section_2_cross_model_performance']['claims']
    review_stats = RESULTS_DIR / "review_statistics"
    
    # SI difficult statements
    claim = section_claims['si_difficult_statements']
    si_matrix = pd.read_csv(review_stats / 'si_model_statement_correctness_matrix.csv', index_col=0)
    si_info = pd.read_csv(review_stats / 'si_statement_info.csv')
    si_miss_rates = 1 - si_matrix.mean(axis=0)
    si_difficult = si_miss_rates[si_miss_rates > 0.5]
    si_difficult_ids = [int(x) for x in si_difficult.index]
    si_difficult_categories = si_info[si_info['statement_index'].isin(si_difficult_ids)]['ground_truth'].value_counts()
    actual_breakdown = dict(si_difficult_categories)
    
    count_match = len(si_difficult) == claim['count']
    pct_match = abs(len(si_difficult)/len(si_info)*100 - claim['percentage']) < 0.5
    breakdown_match = all(
        actual_breakdown.get(k, 0) == v 
        for k, v in claim['breakdown'].items()
    )
    passed = count_match and pct_match and breakdown_match
    
    output.append(f"{'✅ PASSED' if passed else '❌ FAILED'}")
    output.append(f"Claim: SI detection: {claim['count']} statements ({claim['percentage']}%) missed by >50% of {claim['total_models']} models")
    output.append(f"  Expected breakdown: {claim['breakdown']}")
    output.append(f"Data: {len(si_difficult)} statements ({len(si_difficult)/len(si_info)*100:.1f}%)")
    output.append(f"  Actual breakdown: {actual_breakdown}")
    output.append("")
    
    # TR difficult statements
    claim = section_claims['tr_difficult_statements']
    tr_matrix = pd.read_csv(review_stats / 'therapy_request_model_statement_correctness_matrix.csv', index_col=0)
    tr_info = pd.read_csv(review_stats / 'therapy_request_statement_info.csv')
    tr_miss_rates = 1 - tr_matrix.mean(axis=0)
    tr_difficult = tr_miss_rates[tr_miss_rates > 0.5]
    tr_difficult_ids = [int(x) for x in tr_difficult.index]
    tr_difficult_categories = tr_info[tr_info['statement_index'].isin(tr_difficult_ids)]['ground_truth'].value_counts()
    actual_breakdown = dict(tr_difficult_categories)
    
    # Map expected breakdown keys to actual category names
    expected_mapping = {
        'declarative_sad': 'Affect-Containing Declarative Statements - Clearly Sad',
        'non_therapeutic_question_sad': 'Affect-Containing Non-Therapeutic Questions - Clearly Sad'
    }
    
    count_match = len(tr_difficult) == claim['count']
    pct_match = abs(len(tr_difficult)/len(tr_info)*100 - claim['percentage']) < 0.2
    breakdown_match = all(
        actual_breakdown.get(expected_mapping.get(k, k), 0) == v 
        for k, v in claim['breakdown'].items()
    )
    passed = count_match and pct_match and breakdown_match
    
    output.append(f"{'✅ PASSED' if passed else '❌ FAILED'}")
    output.append(f"Claim: Therapy request: {claim['count']} statements ({claim['percentage']}%) missed by >50%")
    output.append(f"  Expected: {claim['breakdown']}")
    output.append(f"Data: {len(tr_difficult)} statements ({len(tr_difficult)/len(tr_info)*100:.1f}%)")
    output.append(f"  Actual: {actual_breakdown}")
    output.append("")
    
    # TE difficult conversations
    claim = section_claims['te_difficult_conversations']
    te_matrix = pd.read_csv(review_stats / 'therapy_engagement_model_conversation_correctness_matrix.csv', index_col=0)
    te_info = pd.read_csv(review_stats / 'therapy_engagement_conversation_info.csv')
    te_miss_rates = 1 - te_matrix.mean(axis=0)
    te_difficult = te_miss_rates[te_miss_rates > 0.5]
    te_difficult_ids = [int(x) for x in te_difficult.index]
    te_difficult_categories = te_info[te_info['statement_index'].isin(te_difficult_ids)]['ground_truth'].value_counts()
    actual_breakdown = dict(te_difficult_categories)
    
    count_match = len(te_difficult) == claim['count']
    pct_match = abs(len(te_difficult)/len(te_info)*100 - claim['percentage']) < 0.3
    breakdown_match = all(
        actual_breakdown.get(k, 0) == v 
        for k, v in claim['breakdown'].items()
    )
    passed = count_match and pct_match and breakdown_match
    
    output.append(f"{'✅ PASSED' if passed else '❌ FAILED'}")
    output.append(f"Claim: Therapy engagement: {claim['count']} conversations ({claim['percentage']}%) missed by >50%")
    output.append(f"  Expected: {claim['breakdown']}")
    output.append(f"Data: {len(te_difficult)} conversations ({len(te_difficult)/len(te_info)*100:.1f}%)")
    output.append(f"  Actual: {actual_breakdown}")
    output.append("")
    
    return output


def verify_section3_p1_p2(output: list, claims: dict, paper_run_dir: Path) -> list:
    """Verify Section 3: P1 and P2 risk estimates"""
    
    output.append("## Section 3: P1 and P2 risk estimates\n")
    
    section_claims = claims['section_3_p1_p2_estimates']['claims']
    
    # Load P1/P2 data
    p1p2_csv = paper_run_dir / "Data" / "processed_data" / "p1_p2_p_harm_values_m_1.0.csv"
    
    if not p1p2_csv.exists():
        output.append("❌ ERROR: P1/P2 values CSV not found")
        output.append(f"  Expected: {p1p2_csv}")
        output.append("  Run the pipeline to generate this file.")
        return output
    
    p1p2_df = pd.read_csv(p1p2_csv)
    
    # Filter to 1% baseline
    baseline_1pct = p1p2_df[p1p2_df['baseline_percentage'] == 1.0]
    p1_data = baseline_1pct[baseline_1pct['risk_type'] == 'P1']
    p2_data = baseline_1pct[baseline_1pct['risk_type'] == 'P2']
    
    p1_values = [p for p in p1_data['risk_probability'].tolist() if p > 1e-15]
    p2_values = [p for p in p2_data['risk_probability'].tolist() if p > 1e-15]
    
    # P1 range claim
    claim = section_claims['p1_range']
    p1_min, p1_max = min(p1_values), max(p1_values)
    p1_orders = np.log10(p1_max) - np.log10(p1_min)
    
    min_match = abs(np.log10(p1_min) - np.log10(claim['min'])) < 0.5
    max_match = abs(np.log10(p1_max) - np.log10(claim['max'])) < 0.5
    orders_match = abs(p1_orders - claim['orders_of_magnitude']) < 1
    passed = min_match and max_match and orders_match
    
    output.append(f"{'✅ PASSED' if passed else '❌ FAILED'}")
    output.append(f"Claim: P1 ranged from {claim['min']:.1e} to {claim['max']:.1e} (~{claim['orders_of_magnitude']} orders, {claim['per_million_min']} to {claim['per_million_max']} per million)")
    output.append(f"Data: {p1_min:.2e} to {p1_max:.2e} ({p1_orders:.1f} orders, {p1_min*1e6:.2f} to {p1_max*1e6:.0f} per million)")
    output.append("")
    
    # Gemma P1 orders claim
    claim = section_claims['gemma_p1_orders']
    gemma_p1s = [row['risk_probability'] for _, row in p1_data.iterrows() 
                 if row['model_family'].lower() == 'gemma' and row['risk_probability'] > 0]
    gemma_p1_range = np.log10(max(gemma_p1s)) - np.log10(min(gemma_p1s))
    passed = abs(gemma_p1_range - claim['value']) < 0.5
    
    output.append(f"{'✅ PASSED' if passed else '❌ FAILED'}")
    output.append(f"Claim: Gemma family P1 varied by {claim['value']} orders of magnitude")
    output.append(f"Data: {gemma_p1_range:.1f} orders of magnitude")
    output.append("")
    
    # Baseline prevalence contribution
    claim = section_claims['baseline_prevalence_contribution']
    prevalence_orders = np.log10(claim['range_max_pct']) - np.log10(claim['range_min_pct'])
    passed = abs(prevalence_orders - claim['value']) < 0.1
    
    output.append(f"{'✅ PASSED' if passed else '❌ FAILED'}")
    output.append(f"Claim: Baseline prevalence ({claim['range_min_pct']}%-{claim['range_max_pct']}%) contributes {claim['value']} orders of magnitude")
    output.append(f"Data: {prevalence_orders:.1f} orders of magnitude")
    output.append("")
    
    # P2 range claim
    claim = section_claims['p2_range']
    p2_min, p2_max = min(p2_values), max(p2_values)
    p2_orders = np.log10(p2_max) - np.log10(p2_min)
    
    min_match = abs(np.log10(p2_min) - np.log10(claim['min'])) < 0.3
    max_match = abs(np.log10(p2_max) - np.log10(claim['max'])) < 0.3
    orders_match = abs(p2_orders - claim['orders_of_magnitude']) < 0.5
    passed = min_match and max_match and orders_match
    
    output.append(f"{'✅ PASSED' if passed else '❌ FAILED'}")
    output.append(f"Claim: P2 ranged from {claim['min']:.1e} to {claim['max']:.1e} (~{claim['orders_of_magnitude']} orders, {claim['per_million_min']} to {claim['per_million_max']} per million)")
    output.append(f"Data: {p2_min:.2e} to {p2_max:.2e} ({p2_orders:.1f} orders, {p2_min*1e6:.0f} to {p2_max*1e6:.0f} per million)")
    output.append("")
    
    # P2 convergence at high m
    claim = section_claims['p2_strong_dependence_convergence']
    high_m_csv = paper_run_dir / "Data" / "processed_data" / f"p1_p2_p_harm_values_m_{float(claim['m_value'])}.csv"
    
    if high_m_csv.exists():
        high_m_df = pd.read_csv(high_m_csv)
        p2_high_m = high_m_df[(high_m_df['baseline_percentage'] == 1.0) & (high_m_df['risk_type'] == 'P2')]
        p2_high_m_mean = p2_high_m['risk_probability'].mean()
        passed = abs(p2_high_m_mean - claim['value']) < 0.001
        
        output.append(f"{'✅ PASSED' if passed else '❌ FAILED'}")
        output.append(f"Claim: P2 converges to {claim['value']:.1e} under strong dependence (m={claim['m_value']})")
        output.append(f"Data: P2 mean = {p2_high_m_mean:.4f}")
    else:
        output.append(f"⚠️ SKIPPED: High-m P2 CSV not found ({high_m_csv.name})")
    output.append("")
    
    # Check for removed claims
    if '_removed_claims' in claims:
        output.append("## Removed Claims (v2 revision)\n")
        for claim_id, claim_info in claims['_removed_claims'].items():
            if claim_id == 'description':
                continue
            output.append(f"ℹ️ REMOVED: {claim_info.get('original_claim', claim_id)}")
            output.append(f"  Reason: {claim_info.get('reason', 'N/A')}")
            output.append("")
    
    return output


def verify_claims(paper_run_dir: Path) -> str:
    """Verify all manuscript claims against pipeline output"""
    
    claims = load_claims()
    
    output = ["# Manuscript Claims Verification\n"]
    output.append(f"Claims file: {CLAIMS_FILE}\n")
    output.append(f"Manuscript version: {claims['_metadata']['manuscript_version']}\n")
    output.append(f"Last updated: {claims['_metadata']['last_updated']}\n")
    output.append("")
    
    # Section 1: Gemini-Generated Synthetic Data
    output = verify_section1_gemini_data(output, claims)
    
    # Section 2: Difficult statements
    output = verify_section2_difficult_statements(output, claims)
    
    # Section 3: P1 and P2 estimates
    output = verify_section3_p1_p2(output, claims, paper_run_dir)
    
    return "\n".join(output)


def main():
    parser = argparse.ArgumentParser(description="Verify manuscript claims against pipeline output")
    parser.add_argument("--paper-run-dir", required=True, help="Path to pipeline output directory")
    parser.add_argument("--output", default="MANUSCRIPT_CLAIMS_VERIFICATION.md", help="Output filename")
    parser.add_argument("--claims-file", default=None, help="Path to claims JSON (default: config/manuscript_claims.json)")
    args = parser.parse_args()
    
    global CLAIMS_FILE
    if args.claims_file:
        CLAIMS_FILE = Path(args.claims_file)
    
    paper_dir = Path(args.paper_run_dir)
    output_path = paper_dir / args.output if paper_dir.is_dir() else Path(args.output)
    
    report = verify_claims(paper_dir)
    
    with open(output_path, 'w') as f:
        f.write(report)
    
    print(f"✓ {output_path}")


if __name__ == "__main__":
    main()
