#!/usr/bin/env python3
"""
Manuscript Claims Verification - Verify paper claims against pipeline output

Reads expected claims from config/manuscript_claims.json and compares against
actual data generated by the regulatory simulation paper pipeline.

Verification approaches:
1. query_cache_directly: Query SQLite cache for API parameters (temperature, max_tokens, top_p)
2. verify_config_and_code: Check that config files contain claimed parameter values
3. computed_from_csv: Verify computed values against freshly-generated pipeline CSVs

Usage:
    python analysis/manuscript_claims_verification.py --paper-run-dir results/REGULATORY_SIMULATION_PAPER/[timestamp]
"""

import pandas as pd
import numpy as np
from pathlib import Path
import argparse
import json
import sqlite3
import sys
from typing import Dict, List, Tuple, Any, Optional
from dataclasses import dataclass, field

ROOT = Path(__file__).parent.parent

# Add project root to path for imports
sys.path.insert(0, str(ROOT))
RESULTS_DIR = ROOT / "results"
CLAIMS_FILE = ROOT / "config" / "manuscript_claims.json"
DEFAULT_CACHE_DIR = ROOT / "regulatory_paper_cache_v3"


@dataclass
class VerificationResult:
    """Result of verifying a single claim."""
    claim_id: str
    claim_text: str
    passed: bool
    expected: str
    actual: str
    details: str = ""
    section: str = ""


def round_to_sig_figs(x: float, sig_figs: int) -> float:
    """Round a number to specified significant figures."""
    if x == 0:
        return 0
    from math import log10, floor
    return round(x, -int(floor(log10(abs(x)))) + (sig_figs - 1))


def format_sig_figs(x: float, sig_figs: int) -> str:
    """Format a number with specified significant figures for display."""
    if x == 0:
        return "0"
    from math import log10, floor
    # Get the order of magnitude
    magnitude = floor(log10(abs(x)))
    # Round to sig figs
    rounded = round_to_sig_figs(x, sig_figs)
    # Format appropriately
    if magnitude >= sig_figs - 1 or magnitude < -3:
        # Use scientific notation
        return f"{rounded:.{sig_figs-1}e}"
    else:
        # Use decimal notation
        decimal_places = max(0, sig_figs - 1 - magnitude)
        return f"{rounded:.{decimal_places}f}"


def check_sig_figs_match(claimed: float, actual: float, sig_figs: int) -> bool:
    """Check if actual rounds to claimed at the specified sig figs."""
    rounded_actual = round_to_sig_figs(actual, sig_figs)
    rounded_claimed = round_to_sig_figs(claimed, sig_figs)
    # Compare with small tolerance for floating point
    if rounded_claimed == 0:
        return rounded_actual == 0
    return abs(rounded_actual - rounded_claimed) / abs(rounded_claimed) < 1e-9


@dataclass
class VerificationReport:
    """Collection of all verification results."""
    results: List[VerificationResult] = field(default_factory=list)
    
    def add(self, result: VerificationResult):
        self.results.append(result)
    
    @property
    def passed_count(self) -> int:
        return sum(1 for r in self.results if r.passed)
    
    @property
    def failed_count(self) -> int:
        return sum(1 for r in self.results if not r.passed)
    
    @property
    def total_count(self) -> int:
        return len(self.results)
    
    def get_failed(self) -> List[VerificationResult]:
        return [r for r in self.results if not r.passed]


def load_claims() -> dict:
    """Load manuscript claims from JSON file."""
    with open(CLAIMS_FILE) as f:
        return json.load(f)


# =============================================================================
# CACHE PARAMETER VERIFICATION
# =============================================================================

def verify_cache_parameters(cache_dir: Path, report: VerificationReport, claims: dict) -> None:
    """
    Query cache directly to verify API parameters for all cached entries.
    
    This verifies claim 2a: temperature=0.0, max_tokens=256, top_p=1.0
    """
    section = "methods"
    claim = claims[section]["claims"]["decoding_params"]
    
    expected_temp = claim["temperature"]
    expected_max_tokens = claim["max_tokens"]
    expected_top_p = claim["top_p"]
    
    db_path = cache_dir / "results.db"
    
    if not db_path.exists():
        report.add(VerificationResult(
            claim_id="decoding_params",
            claim_text=claim["claim_text"],
            passed=False,
            expected=f"temp={expected_temp}, max_tokens={expected_max_tokens}, top_p={expected_top_p}",
            actual="CACHE NOT FOUND",
            details=f"Cache database not found at {db_path}",
            section=section
        ))
        return
    
    with sqlite3.connect(db_path) as conn:
        # Query all unique parameter combinations in cache_keys
        cursor = conn.execute("""
            SELECT DISTINCT temperature, max_tokens, top_p, COUNT(*) as count
            FROM cache_keys
            GROUP BY temperature, max_tokens, top_p
        """)
        param_combinations = cursor.fetchall()
        
        # Get total entry count
        cursor = conn.execute("SELECT COUNT(*) FROM cache_keys")
        total_entries = cursor.fetchone()[0]
    
    # Check if all entries have correct parameters
    all_correct = True
    incorrect_entries = 0
    actual_params = []
    
    for temp, max_tok, top_p, count in param_combinations:
        actual_params.append(f"temp={temp}, max_tokens={max_tok}, top_p={top_p} ({count} entries)")
        if temp != expected_temp or max_tok != expected_max_tokens or top_p != expected_top_p:
            all_correct = False
            incorrect_entries += count
    
    passed = all_correct
    
    report.add(VerificationResult(
        claim_id="decoding_params",
        claim_text=claim["claim_text"],
        passed=passed,
        expected=f"temp={expected_temp}, max_tokens={expected_max_tokens}, top_p={expected_top_p} (all {total_entries} entries)",
        actual="; ".join(actual_params) if actual_params else "No entries found",
        details=f"Queried cache_keys table directly. {incorrect_entries} entries with incorrect params." if not passed else f"All {total_entries} cache entries verified.",
        section=section
    ))


# =============================================================================
# CONFIG/CODE PARAMETER VERIFICATION
# =============================================================================

def verify_config_parameters(report: VerificationReport, claims: dict) -> None:
    """Verify that config files contain claimed parameter values."""
    
    # Import config to check values
    from config.regulatory_paper_parameters import RISK_MODEL_PARAMS, API_PARAMS
    
    section = "methods"
    
    # Claim 2b: Confidence intervals - verify BOTH 95% CI level AND bootstrap replicates
    claim_2b = claims[section]["claims"]["confidence_intervals"]
    expected_bootstrap = claim_2b["n_bootstrap_replicates"]
    expected_ci_level = claim_2b["ci_level"]  # 0.95
    expected_alpha = 1 - expected_ci_level  # 0.05
    
    import re
    
    # Check bootstrap value in plotting code
    bootstrap_code_path = ROOT / "analysis" / "comparative_analysis" / "multi_experiment_plot_transposed_provenance.py"
    actual_bootstrap = None
    if bootstrap_code_path.exists():
        with open(bootstrap_code_path) as f:
            content = f.read()
            match = re.search(r'n_bootstrap\s*=\s*(\d+)', content)
            if match:
                actual_bootstrap = int(match.group(1))
    
    # Check alpha value in statistics.py (clopper_pearson_ci default alpha)
    stats_code_path = ROOT / "utilities" / "statistics.py"
    actual_alpha = None
    if stats_code_path.exists():
        with open(stats_code_path) as f:
            content = f.read()
            # Look for alpha: float = 0.05 in clopper_pearson_ci
            match = re.search(r'def clopper_pearson_ci.*?alpha:\s*float\s*=\s*([\d.]+)', content, re.DOTALL)
            if match:
                actual_alpha = float(match.group(1))
    
    bootstrap_match = actual_bootstrap == expected_bootstrap
    alpha_match = abs(actual_alpha - expected_alpha) < 1e-10 if actual_alpha is not None else False
    
    report.add(VerificationResult(
        claim_id="confidence_intervals",
        claim_text=claim_2b["claim_text"][:100] + "...",
        passed=bootstrap_match and alpha_match,
        expected=f"n_bootstrap={expected_bootstrap}, alpha={expected_alpha} (95% CI)",
        actual=f"n_bootstrap={actual_bootstrap}, alpha={actual_alpha}",
        details=f"Checked {bootstrap_code_path.name} and {stats_code_path.name}",
        section=section
    ))
    
    # Claim 2c: LOD CI level (Beta floor)
    claim_2c = claims[section]["claims"]["beta_floor"]
    expected_lod = claim_2c["lod_ci_level"]
    actual_lod = RISK_MODEL_PARAMS.get("lod_ci_level")
    
    report.add(VerificationResult(
        claim_id="beta_floor",
        claim_text=claim_2c["claim_text"][:100] + "...",
        passed=actual_lod == expected_lod,
        expected=f"lod_ci_level={expected_lod}",
        actual=f"lod_ci_level={actual_lod}",
        details="Checked config/regulatory_paper_parameters.py",
        section=section
    ))
    
    # Claim 2d: Monte Carlo samples
    claim_2d = claims[section]["claims"]["monte_carlo"]
    expected_mc = claim_2d["n_mc_samples"]
    actual_mc = RISK_MODEL_PARAMS.get("n_mc_samples")
    
    report.add(VerificationResult(
        claim_id="monte_carlo",
        claim_text=claim_2d["claim_text"][:100] + "...",
        passed=actual_mc == expected_mc,
        expected=f"n_mc_samples={expected_mc}",
        actual=f"n_mc_samples={actual_mc}",
        details="Checked config/regulatory_paper_parameters.py",
        section=section
    ))
    
    # Claim 3d_i: Failure multiplier range
    section_3d = "results"
    claim_3d_i = claims[section_3d]["correlated_failure"]["claims"]["failure_multiplier_range"]
    expected_m_values = [claim_3d_i["m_min"], claim_3d_i["m_max"]]
    actual_m_values = RISK_MODEL_PARAMS.get("failure_multiplier_values", [])
    
    m_range_correct = (
        len(actual_m_values) > 0 and
        min(actual_m_values) == expected_m_values[0] and
        max(actual_m_values) == expected_m_values[1]
    )
    
    report.add(VerificationResult(
        claim_id="failure_multiplier_range",
        claim_text=claim_3d_i["claim_text"],
        passed=m_range_correct,
        expected=f"m range: {expected_m_values[0]} to {expected_m_values[1]}",
        actual=f"m range: {min(actual_m_values) if actual_m_values else 'N/A'} to {max(actual_m_values) if actual_m_values else 'N/A'}",
        details="Checked config/regulatory_paper_parameters.py:failure_multiplier_values",
        section="results/correlated_failure"
    ))


# =============================================================================
# SECTION 1: ABSTRACT CLAIMS
# =============================================================================

def verify_abstract_claims(report: VerificationReport, claims: dict, paper_run_dir: Path) -> None:
    """Verify abstract claims (P1/P2 ranges)."""
    
    section = "abstract"
    section_claims = claims[section]["claims"]
    
    # Load P1/P2 data
    p1p2_csv = paper_run_dir / "Data" / "processed_data" / "correlated_failure_analysis" / "p1_p2_p_harm_values_m_1.0.csv"
    
    if not p1p2_csv.exists():
        for claim_id in ["p1_range", "p2_range", "orders_of_magnitude"]:
            report.add(VerificationResult(
                claim_id=claim_id,
                claim_text=section_claims[claim_id]["claim_text"],
                passed=False,
                expected="Data file",
                actual="FILE NOT FOUND",
                details=f"P1/P2 CSV not found at {p1p2_csv}",
                section=section
            ))
        return
    
    p1p2_df = pd.read_csv(p1p2_csv)
    baseline_1pct = p1p2_df[p1p2_df['baseline_percentage'] == 1.0]
    
    p1_data = baseline_1pct[baseline_1pct['risk_type'] == 'P1']
    p2_data = baseline_1pct[baseline_1pct['risk_type'] == 'P2']
    
    p1_values = [p for p in p1_data['risk_probability'].tolist() if p > 1e-15]
    p2_values = [p for p in p2_data['risk_probability'].tolist() if p > 1e-15]
    
    # Claim 1a: P1 range
    # Use 2 significant figures matching
    claim = section_claims["p1_range"]
    p1_min, p1_max = min(p1_values), max(p1_values)
    
    sig_figs = 2
    min_match = check_sig_figs_match(claim["min"], p1_min, sig_figs)
    max_match = check_sig_figs_match(claim["max"], p1_max, sig_figs)
    
    report.add(VerificationResult(
        claim_id="p1_range",
        claim_text=claim["claim_text"],
        passed=min_match and max_match,
        expected=f"{claim['min']:.1e} to {claim['max']:.1e}",
        actual=f"{p1_min:.2e} to {p1_max:.2e} (rounded: {format_sig_figs(p1_min, sig_figs)} to {format_sig_figs(p1_max, sig_figs)})",
        details=f"At 1% baseline SI prevalence. Using {sig_figs} sig figs.",
        section=section
    ))
    
    # Claim 1b: P2 range
    # Use 2 significant figures matching
    claim = section_claims["p2_range"]
    p2_min, p2_max = min(p2_values), max(p2_values)
    
    sig_figs = 2
    min_match = check_sig_figs_match(claim["min"], p2_min, sig_figs)
    max_match = check_sig_figs_match(claim["max"], p2_max, sig_figs)
    
    report.add(VerificationResult(
        claim_id="p2_range",
        claim_text=claim["claim_text"],
        passed=min_match and max_match,
        expected=f"{claim['min']:.1e} to {claim['max']:.1e}",
        actual=f"{p2_min:.2e} to {p2_max:.2e} (rounded: {format_sig_figs(p2_min, sig_figs)} to {format_sig_figs(p2_max, sig_figs)})",
        details=f"At 1% baseline P(lack care -> harm). Using {sig_figs} sig figs.",
        section=section
    ))
    
    # Claim 1c: Orders of magnitude (largest span)
    claim = section_claims["orders_of_magnitude"]
    p1_orders = np.log10(p1_max) - np.log10(p1_min)
    p2_orders = np.log10(p2_max) - np.log10(p2_min)
    max_orders = max(p1_orders, p2_orders)
    
    report.add(VerificationResult(
        claim_id="orders_of_magnitude",
        claim_text=claim["claim_text"],
        passed=abs(max_orders - claim["value"]) < 1,
        expected=f"~{claim['value']} orders of magnitude",
        actual=f"{max_orders:.1f} orders (P1: {p1_orders:.1f}, P2: {p2_orders:.1f})",
        details="Largest span for either P1 or P2",
        section=section
    ))


# =============================================================================
# SECTION 3a: SYNTHETIC DATA CLAIMS
# =============================================================================

def verify_synthetic_data_claims(report: VerificationReport, claims: dict) -> None:
    """Verify claims about Gemini-generated synthetic data retention rates."""
    
    section = "results"
    subsection = "synthetic_data"
    section_claims = claims[section][subsection]["claims"]
    
    review_stats_dir = RESULTS_DIR / "review_statistics"
    
    # Load review statistics
    try:
        si_stats = pd.read_csv(review_stats_dir / 'si_review_statistics.csv')
        tr_stats = pd.read_csv(review_stats_dir / 'therapy_request_review_statistics.csv')
        te_stats = pd.read_csv(review_stats_dir / 'therapy_engagement_review_statistics.csv')
        chi_sq = pd.read_csv(review_stats_dir / 'chi_squared_tests.csv')
    except FileNotFoundError as e:
        for claim_id in section_claims.keys():
            report.add(VerificationResult(
                claim_id=claim_id,
                claim_text=section_claims[claim_id]["claim_text"][:80] + "...",
                passed=False,
                expected="Data file",
                actual="FILE NOT FOUND",
                details=f"Review statistics not found: {e}",
                section=f"{section}/{subsection}"
            ))
        return
    
    # Claim 3a_i: Non-SI retention
    claim = section_claims["non_si_retained"]
    non_si_row = si_stats[si_stats['Category'] == 'SUBTOTAL: Non-SI'].iloc[0]
    actual_num = non_si_row['Approved (No Changes)']
    actual_denom = non_si_row['Generated']
    
    report.add(VerificationResult(
        claim_id="non_si_retained",
        claim_text=claim["claim_text"],
        passed=(actual_num == claim["numerator"] and actual_denom == claim["denominator"]),
        expected=f"{claim['numerator']}/{claim['denominator']}",
        actual=f"{actual_num}/{actual_denom}",
        details="Non-suicidal statements retained without changes",
        section=f"{section}/{subsection}"
    ))
    
    # Claim 3a_ii: SI retention with p-value
    claim = section_claims["si_retention"]
    si_row = si_stats[si_stats['Category'] == 'SUBTOTAL: SI'].iloc[0]
    actual_num = si_row['Approved (No Changes)']
    actual_denom = si_row['Generated']
    
    si_chi = chi_sq[chi_sq['Comparison'] == 'Non-SI vs SI'].iloc[0]
    p_value = si_chi['P_Value']
    
    report.add(VerificationResult(
        claim_id="si_retention",
        claim_text=claim["claim_text"],
        passed=(actual_num == claim["numerator"] and actual_denom == claim["denominator"] and p_value < claim["p_value_threshold"]),
        expected=f"{claim['numerator']}/{claim['denominator']}, p < {claim['p_value_threshold']}",
        actual=f"{actual_num}/{actual_denom}, p = {p_value:.2e}",
        details="SI content retention with chi-squared test",
        section=f"{section}/{subsection}"
    ))
    
    # Claim 3a_iii: Therapy request retention
    claim = section_claims["therapy_request_retention"]
    
    # Non-therapeutic = Declarative + Non-Therapeutic Questions
    tr_decl = tr_stats[tr_stats['Category'] == 'SUBTOTAL: Declarative Statements'].iloc[0]
    tr_nonther = tr_stats[tr_stats['Category'] == 'SUBTOTAL: Non-Therapeutic Questions'].iloc[0]
    actual_nonther_num = tr_decl['Approved (No Changes)'] + tr_nonther['Approved (No Changes)']
    actual_nonther_denom = tr_decl['Generated'] + tr_nonther['Generated']
    
    tr_req = tr_stats[tr_stats['Category'] == 'SUBTOTAL: Explicit Therapy Requests'].iloc[0]
    actual_req_num = tr_req['Approved (No Changes)']
    actual_req_denom = tr_req['Generated']
    
    tr_chi = chi_sq[chi_sq['Comparison'] == 'No Therapy Request vs Therapy Request'].iloc[0]
    p_value = tr_chi['P_Value']
    
    nonther_match = (actual_nonther_num == claim["non_therapeutic"]["numerator"] and 
                    actual_nonther_denom == claim["non_therapeutic"]["denominator"])
    req_match = (actual_req_num == claim["explicit_requests"]["numerator"] and 
                actual_req_denom == claim["explicit_requests"]["denominator"])
    p_match = p_value < claim["p_value_threshold"]
    
    report.add(VerificationResult(
        claim_id="therapy_request_retention",
        claim_text=claim["claim_text"][:80] + "...",
        passed=nonther_match and req_match and p_match,
        expected=f"Non-ther: {claim['non_therapeutic']['numerator']}/{claim['non_therapeutic']['denominator']}, Explicit: {claim['explicit_requests']['numerator']}/{claim['explicit_requests']['denominator']}, p < {claim['p_value_threshold']}",
        actual=f"Non-ther: {actual_nonther_num}/{actual_nonther_denom}, Explicit: {actual_req_num}/{actual_req_denom}, p = {p_value:.2e}",
        details="Therapy request retention rates",
        section=f"{section}/{subsection}"
    ))
    
    # Claim 3a_iv: Therapy engagement retention
    claim = section_claims["therapy_engagement_retention"]
    
    te_nonther = te_stats[te_stats['Category'] == 'SUBTOTAL: Non-Therapeutic Conversations'].iloc[0]
    te_amb = te_stats[te_stats['Category'] == 'SUBTOTAL: Ambiguous Engagement'].iloc[0]
    te_ther = te_stats[te_stats['Category'] == 'SUBTOTAL: Therapeutic Conversation'].iloc[0]
    
    nonther_amb_approved = te_nonther['Approved (No Changes)'] + te_amb['Approved (No Changes)']
    nonther_amb_total = te_nonther['Generated'] + te_amb['Generated']
    nonther_amb_pct = nonther_amb_approved / nonther_amb_total * 100
    
    ther_approved = te_ther['Approved (No Changes)']
    ther_total = te_ther['Generated']
    ther_pct = ther_approved / ther_total * 100
    
    te_chi = chi_sq[chi_sq['Comparison'] == 'Non-Therapeutic Interaction vs Therapeutic Conversation'].iloc[0]
    p_value = te_chi['P_Value']
    
    # "Nearly all" = >=90%, "fewer than half" = <50%
    nonther_amb_pass = nonther_amb_pct >= claim["non_therapeutic_ambiguous_threshold_pct"]
    ther_pass = ther_pct < claim["therapeutic_threshold_pct"]
    p_pass = p_value < claim["p_value_threshold"]
    
    report.add(VerificationResult(
        claim_id="therapy_engagement_retention",
        claim_text=claim["claim_text"][:80] + "...",
        passed=nonther_amb_pass and ther_pass and p_pass,
        expected=f"Non-ther/amb >= {claim['non_therapeutic_ambiguous_threshold_pct']}%, Ther < {claim['therapeutic_threshold_pct']}%, p < {claim['p_value_threshold']}",
        actual=f"Non-ther/amb: {nonther_amb_pct:.1f}% ({nonther_amb_approved}/{nonther_amb_total}), Ther: {ther_pct:.1f}% ({ther_approved}/{ther_total}), p = {p_value:.2e}",
        details="'Nearly all' = >=90%, 'fewer than half' = <50%",
        section=f"{section}/{subsection}"
    ))
    
    # Claim 3a_v: CBT/DBT accuracy
    claim = section_claims["cbt_dbt_accuracy"]
    
    # CBT/DBT categories
    cbt_cog = te_stats[te_stats['Category'] == 'Therapeutic: Cognitive Technique'].iloc[0]
    cbt_skill = te_stats[te_stats['Category'] == 'Therapeutic: CBT/DBT Skill'].iloc[0]
    cbt_approved = cbt_cog['Approved (No Changes)'] + cbt_skill['Approved (No Changes)']
    cbt_total = cbt_cog['Generated'] + cbt_skill['Generated']
    cbt_pct = cbt_approved / cbt_total * 100
    
    # Psychodynamic/diagnosis/medication categories
    psych = te_stats[te_stats['Category'] == 'Therapeutic: Psychodynamic'].iloc[0]
    diag = te_stats[te_stats['Category'] == 'Therapeutic: Diagnosis'].iloc[0]
    med = te_stats[te_stats['Category'] == 'Therapeutic: Med Recommendation'].iloc[0]
    
    psych_pct = psych['Approved (No Changes)'] / psych['Generated'] * 100
    diag_pct = diag['Approved (No Changes)'] / diag['Generated'] * 100
    med_pct = med['Approved (No Changes)'] / med['Generated'] * 100
    
    # "Largely accurate" = >=75%, "frequently broke frame" = <50%
    cbt_pass = cbt_pct >= claim["cbt_dbt_threshold_pct"]
    pdm_pass = (psych_pct < claim["psychodynamic_diagnosis_medication_threshold_pct"] and
               diag_pct < claim["psychodynamic_diagnosis_medication_threshold_pct"] and
               med_pct < claim["psychodynamic_diagnosis_medication_threshold_pct"])
    
    report.add(VerificationResult(
        claim_id="cbt_dbt_accuracy",
        claim_text=claim["claim_text"][:80] + "...",
        passed=cbt_pass and pdm_pass,
        expected=f"CBT/DBT >= {claim['cbt_dbt_threshold_pct']}%, Psych/Diag/Med < {claim['psychodynamic_diagnosis_medication_threshold_pct']}%",
        actual=f"CBT/DBT: {cbt_pct:.1f}%, Psych: {psych_pct:.1f}%, Diag: {diag_pct:.1f}%, Med: {med_pct:.1f}%",
        details="'Largely accurate' = >=75%, 'frequently broke frame' = <50%",
        section=f"{section}/{subsection}"
    ))


# =============================================================================
# SECTION 3b: MODEL PERFORMANCE CLAIMS
# =============================================================================

def verify_model_performance_claims(report: VerificationReport, claims: dict, paper_run_dir: Path) -> None:
    """Verify claims about model performance characteristics."""
    
    section = "results"
    subsection = "model_performance"
    section_claims = claims[section][subsection]["claims"]
    
    review_stats_dir = RESULTS_DIR / "review_statistics"
    metrics_dir = paper_run_dir / "Data" / "processed_data" / "model_performance_metrics"
    
    # Claim 3b_i: Lower-parameter models struggled
    claim = section_claims["lower_param_struggled"]
    
    # Load metrics to check parse rates by model
    try:
        # Try to load comprehensive metrics
        metrics_files = list(metrics_dir.glob("*comprehensive_metrics.csv"))
        if metrics_files:
            all_metrics = []
            for f in metrics_files:
                df = pd.read_csv(f)
                all_metrics.append(df)
            metrics_df = pd.concat(all_metrics, ignore_index=True) if all_metrics else pd.DataFrame()
        else:
            metrics_df = pd.DataFrame()
    except Exception as e:
        metrics_df = pd.DataFrame()
    
    if metrics_df.empty or 'model_family' not in metrics_df.columns:
        report.add(VerificationResult(
            claim_id="lower_param_struggled",
            claim_text=claim["claim_text"][:80] + "...",
            passed=False,
            expected="Metrics data",
            actual="DATA NOT FOUND",
            details=f"Could not load metrics from {metrics_dir}",
            section=f"{section}/{subsection}"
        ))
    else:
        # Check if smallest model in each family has < threshold parse rate
        families_failing = 0
        family_details = []
        threshold = claim["parse_rate_threshold_pct"] / 100  # Convert to decimal
        
        for family in claim["families_to_check"]:
            # Use startswith for matching (handles llama3.1, llama3.2, etc.)
            family_data = metrics_df[metrics_df['model_family'].str.lower().str.startswith(family.lower())]
            if len(family_data) == 0:
                family_details.append(f"{family}: no data")
                continue
            
            # Check parse_success_rate column
            parse_col = 'parse_success_rate' if 'parse_success_rate' in family_data.columns else 'parse_rate'
            if parse_col in family_data.columns:
                # Get smallest model's parse rate (min value indicates struggling)
                smallest_parse = family_data[parse_col].min()
                
                if smallest_parse < threshold:
                    families_failing += 1
                    family_details.append(f"{family}: smallest={smallest_parse:.1%} < {threshold:.0%}")
                else:
                    family_details.append(f"{family}: smallest={smallest_parse:.1%} >= {threshold:.0%} (OK)")
        
        report.add(VerificationResult(
            claim_id="lower_param_struggled",
            claim_text=claim["claim_text"][:80] + "...",
            passed=families_failing >= claim["min_families_failing"],
            expected=f">={claim['min_families_failing']} of {len(claim['families_to_check'])} families with smallest model < {claim['parse_rate_threshold_pct']}% parse rate",
            actual=f"{families_failing} families failing. {'; '.join(family_details)}",
            details=f"Checking {', '.join(claim['families_to_check'])} families",
            section=f"{section}/{subsection}"
        ))
    
    # Claim 3b_ii: Performance improved with size
    claim = section_claims["performance_improved_with_size"]
    
    if metrics_df.empty:
        report.add(VerificationResult(
            claim_id="performance_improved_with_size",
            claim_text=claim["claim_text"][:80] + "...",
            passed=False,
            expected="Metrics data",
            actual="DATA NOT FOUND",
            details="Could not load metrics",
            section=f"{section}/{subsection}"
        ))
    else:
        # Actually compute Spearman correlation between model size and metrics
        # Parse model size to numeric (e.g., "270m-it" -> 0.27, "1b-it" -> 1.0, "70b" -> 70.0)
        def parse_size_to_billions(size_str: str) -> float:
            size_str = size_str.lower().replace('-it', '').replace('-bartowski', '').replace('-q8', '')
            if 'm' in size_str:
                return float(size_str.replace('m', '')) / 1000
            elif 'b' in size_str:
                return float(size_str.replace('b', ''))
            return 0
        
        metrics_df['size_billions'] = metrics_df['model_size'].apply(parse_size_to_billions)
        
        # Check each metric for positive correlation across families
        metrics_to_check = ['sensitivity', 'specificity', 'accuracy', 'f1_score']
        families_with_positive_corr = 0
        total_families = 0
        correlation_details = []
        
        for family in metrics_df['model_family'].str.lower().unique():
            # Handle llama family variations
            if family.startswith('llama'):
                family_data = metrics_df[metrics_df['model_family'].str.lower().str.startswith('llama')]
            else:
                family_data = metrics_df[metrics_df['model_family'].str.lower() == family]
            
            if len(family_data) < 3:  # Need at least 3 points for meaningful correlation
                continue
            
            total_families += 1
            positive_metrics = 0
            
            for metric in metrics_to_check:
                if metric in family_data.columns:
                    from scipy.stats import spearmanr
                    corr, _ = spearmanr(family_data['size_billions'], family_data[metric])
                    if corr > 0:
                        positive_metrics += 1
            
            # Family shows positive correlation if majority of metrics are positive
            if positive_metrics >= len(metrics_to_check) / 2:
                families_with_positive_corr += 1
                correlation_details.append(f"{family}: positive ({positive_metrics}/{len(metrics_to_check)} metrics)")
            else:
                correlation_details.append(f"{family}: NOT positive ({positive_metrics}/{len(metrics_to_check)} metrics)")
        
        pct_positive = (families_with_positive_corr / total_families * 100) if total_families > 0 else 0
        passed = pct_positive >= claim["threshold_pct"]
        
        report.add(VerificationResult(
            claim_id="performance_improved_with_size",
            claim_text=claim["claim_text"][:80] + "...",
            passed=passed,
            expected=f">={claim['threshold_pct']}% of families show positive correlation",
            actual=f"{pct_positive:.0f}% ({families_with_positive_corr}/{total_families} families). {'; '.join(correlation_details)}",
            details="Computed Spearman correlation between model size and metrics",
            section=f"{section}/{subsection}"
        ))
    
    # Claim 3b_iii: SI difficult statements
    claim = section_claims["si_difficult_statements"]
    try:
        si_matrix = pd.read_csv(review_stats_dir / 'si_model_statement_correctness_matrix.csv', index_col=0)
        si_info = pd.read_csv(review_stats_dir / 'si_statement_info.csv')
        
        si_miss_rates = 1 - si_matrix.mean(axis=0)
        si_difficult = si_miss_rates[si_miss_rates > claim["threshold_miss_rate"]]
        si_difficult_ids = [int(x) for x in si_difficult.index]
        si_difficult_categories = si_info[si_info['statement_index'].isin(si_difficult_ids)]['ground_truth'].value_counts()
        actual_breakdown = dict(si_difficult_categories)
        
        count_match = len(si_difficult) == claim["count"]
        pct_match = abs(len(si_difficult)/len(si_info)*100 - claim["percentage"]) < 0.5
        
        report.add(VerificationResult(
            claim_id="si_difficult_statements",
            claim_text=claim["claim_text"][:80] + "...",
            passed=count_match and pct_match,
            expected=f"{claim['count']} statements ({claim['percentage']}%), breakdown: {claim['breakdown']}",
            actual=f"{len(si_difficult)} statements ({len(si_difficult)/len(si_info)*100:.1f}%), breakdown: {actual_breakdown}",
            details=f"Statements missed by >{claim['threshold_miss_rate']*100}% of {claim['total_models']} models",
            section=f"{section}/{subsection}"
        ))
    except FileNotFoundError as e:
        report.add(VerificationResult(
            claim_id="si_difficult_statements",
            claim_text=claim["claim_text"][:80] + "...",
            passed=False,
            expected="Matrix file",
            actual="FILE NOT FOUND",
            details=str(e),
            section=f"{section}/{subsection}"
        ))
    
    # Claim 3b_iv: TR difficult statements
    claim = section_claims["tr_difficult_statements"]
    try:
        tr_matrix = pd.read_csv(review_stats_dir / 'therapy_request_model_statement_correctness_matrix.csv', index_col=0)
        tr_info = pd.read_csv(review_stats_dir / 'therapy_request_statement_info.csv')
        
        tr_miss_rates = 1 - tr_matrix.mean(axis=0)
        tr_difficult = tr_miss_rates[tr_miss_rates > claim["threshold_miss_rate"]]
        
        count_match = len(tr_difficult) == claim["count"]
        pct_match = abs(len(tr_difficult)/len(tr_info)*100 - claim["percentage"]) < 0.2
        
        report.add(VerificationResult(
            claim_id="tr_difficult_statements",
            claim_text=claim["claim_text"][:80] + "...",
            passed=count_match and pct_match,
            expected=f"{claim['count']} statements ({claim['percentage']}%)",
            actual=f"{len(tr_difficult)} statements ({len(tr_difficult)/len(tr_info)*100:.1f}%)",
            details=f"Statements missed by >{claim['threshold_miss_rate']*100}% of models",
            section=f"{section}/{subsection}"
        ))
    except FileNotFoundError as e:
        report.add(VerificationResult(
            claim_id="tr_difficult_statements",
            claim_text=claim["claim_text"][:80] + "...",
            passed=False,
            expected="Matrix file",
            actual="FILE NOT FOUND",
            details=str(e),
            section=f"{section}/{subsection}"
        ))
    
    # Claim 3b_v: TE difficult conversations
    claim = section_claims["te_difficult_conversations"]
    try:
        te_matrix = pd.read_csv(review_stats_dir / 'therapy_engagement_model_conversation_correctness_matrix.csv', index_col=0)
        te_info = pd.read_csv(review_stats_dir / 'therapy_engagement_conversation_info.csv')
        
        te_miss_rates = 1 - te_matrix.mean(axis=0)
        te_difficult = te_miss_rates[te_miss_rates > claim["threshold_miss_rate"]]
        
        count_match = len(te_difficult) == claim["count"]
        pct_match = abs(len(te_difficult)/len(te_info)*100 - claim["percentage"]) < 0.3
        
        report.add(VerificationResult(
            claim_id="te_difficult_conversations",
            claim_text=claim["claim_text"][:80] + "...",
            passed=count_match and pct_match,
            expected=f"{claim['count']} conversations ({claim['percentage']}%)",
            actual=f"{len(te_difficult)} conversations ({len(te_difficult)/len(te_info)*100:.1f}%)",
            details=f"Conversations missed by >{claim['threshold_miss_rate']*100}% of models",
            section=f"{section}/{subsection}"
        ))
    except FileNotFoundError as e:
        report.add(VerificationResult(
            claim_id="te_difficult_conversations",
            claim_text=claim["claim_text"][:80] + "...",
            passed=False,
            expected="Matrix file",
            actual="FILE NOT FOUND",
            details=str(e),
            section=f"{section}/{subsection}"
        ))


# =============================================================================
# SECTION 3c: P1/P2 CLAIMS
# =============================================================================

def verify_p1_p2_claims(report: VerificationReport, claims: dict, paper_run_dir: Path) -> None:
    """Verify P1 and P2 risk estimate claims."""
    
    section = "results"
    subsection = "p1_p2"
    section_claims = claims[section][subsection]["claims"]
    
    p1p2_csv = paper_run_dir / "Data" / "processed_data" / "correlated_failure_analysis" / "p1_p2_p_harm_values_m_1.0.csv"
    
    if not p1p2_csv.exists():
        for claim_id in section_claims.keys():
            report.add(VerificationResult(
                claim_id=claim_id,
                claim_text=section_claims[claim_id]["claim_text"][:80] + "...",
                passed=False,
                expected="Data file",
                actual="FILE NOT FOUND",
                details=f"P1/P2 CSV not found at {p1p2_csv}",
                section=f"{section}/{subsection}"
            ))
        return
    
    p1p2_df = pd.read_csv(p1p2_csv)
    baseline_1pct = p1p2_df[p1p2_df['baseline_percentage'] == 1.0]
    
    p1_data = baseline_1pct[baseline_1pct['risk_type'] == 'P1']
    p2_data = baseline_1pct[baseline_1pct['risk_type'] == 'P2']
    
    p1_values = [p for p in p1_data['risk_probability'].tolist() if p > 1e-15]
    p2_values = [p for p in p2_data['risk_probability'].tolist() if p > 1e-15]
    
    # Claim 3c_i: P1 range
    # Use 2 significant figures matching
    claim = section_claims["p1_range"]
    p1_min, p1_max = min(p1_values), max(p1_values)
    p1_orders = np.log10(p1_max) - np.log10(p1_min)
    per_million_min = p1_min * 1e6
    per_million_max = p1_max * 1e6
    
    sig_figs = 2
    min_match = check_sig_figs_match(claim["min"], p1_min, sig_figs)
    max_match = check_sig_figs_match(claim["max"], p1_max, sig_figs)
    orders_match = round(p1_orders) == claim["orders_of_magnitude"]  # Orders should match when rounded
    per_million_min_match = check_sig_figs_match(claim["per_million_min"], per_million_min, sig_figs)
    per_million_max_match = check_sig_figs_match(claim["per_million_max"], per_million_max, 3)  # 3 sig figs for 153
    
    report.add(VerificationResult(
        claim_id="p1_range",
        claim_text=claim["claim_text"][:80] + "...",
        passed=min_match and max_match and orders_match and per_million_min_match and per_million_max_match,
        expected=f"{claim['min']:.1e} to {claim['max']:.1e}, {claim['orders_of_magnitude']} orders, {claim['per_million_min']} to {claim['per_million_max']} per million",
        actual=f"{format_sig_figs(p1_min, sig_figs)} to {format_sig_figs(p1_max, sig_figs)}, {round(p1_orders)} orders, {format_sig_figs(per_million_min, sig_figs)} to {format_sig_figs(per_million_max, 3)} per million",
        details=f"At 1% baseline SI prevalence. Using {sig_figs} sig figs. min_match={min_match}, max_match={max_match}, orders_match={orders_match}, per_million_min_match={per_million_min_match}, per_million_max_match={per_million_max_match}",
        section=f"{section}/{subsection}"
    ))
    
    # Claim 3c_ii: P1 uncertainty pattern
    claim = section_claims["p1_uncertainty_pattern"]
    
    # Check if CI columns exist (try both naming conventions)
    ci_lower_col = 'risk_ci_5' if 'risk_ci_5' in p1_data.columns else 'ci_lower'
    ci_upper_col = 'risk_ci_95' if 'risk_ci_95' in p1_data.columns else 'ci_upper'
    
    if ci_lower_col in p1_data.columns and ci_upper_col in p1_data.columns:
        p1_with_ci = p1_data.copy()
        p1_with_ci['ci_width'] = p1_with_ci[ci_upper_col] - p1_with_ci[ci_lower_col]
        p1_with_ci['ci_width_pct'] = p1_with_ci['ci_width'] / p1_with_ci['risk_probability'] * 100
        
        # Compare lowest quartile to highest quartile
        q25 = p1_with_ci['risk_probability'].quantile(0.25)
        q75 = p1_with_ci['risk_probability'].quantile(0.75)
        
        low_quartile = p1_with_ci[p1_with_ci['risk_probability'] <= q25]['ci_width_pct'].mean()
        high_quartile = p1_with_ci[p1_with_ci['risk_probability'] >= q75]['ci_width_pct'].mean()
        
        report.add(VerificationResult(
            claim_id="p1_uncertainty_pattern",
            claim_text=claim["claim_text"][:80] + "...",
            passed=low_quartile > high_quartile,
            expected="CI width % larger for low-P1 models than high-P1 models",
            actual=f"Low quartile: {low_quartile:.1f}% CI width, High quartile: {high_quartile:.1f}% CI width",
            details="Comparing CI width as % of point estimate",
            section=f"{section}/{subsection}"
        ))
    else:
        report.add(VerificationResult(
            claim_id="p1_uncertainty_pattern",
            claim_text=claim["claim_text"][:80] + "...",
            passed=False,
            expected="CI columns",
            actual="CI columns not found in CSV",
            details="Need risk_ci_5/risk_ci_95 or ci_lower/ci_upper columns",
            section=f"{section}/{subsection}"
        ))
    
    # Claim 3c_iii: Gemma P1 orders and baseline contribution
    # Use 2 significant figures matching
    claim = section_claims["gemma_p1_and_baseline"]
    
    gemma_p1s = [row['risk_probability'] for _, row in p1_data.iterrows() 
                 if 'gemma' in str(row.get('model_family', '')).lower() and row['risk_probability'] > 0]
    
    if gemma_p1s:
        gemma_p1_orders = np.log10(max(gemma_p1s)) - np.log10(min(gemma_p1s))
        baseline_orders = np.log10(claim["baseline_range_max_pct"]) - np.log10(claim["baseline_range_min_pct"])
        
        # Round to 1 decimal place (as stated in claim: 3.4 orders)
        gemma_orders_rounded = round(gemma_p1_orders, 1)
        baseline_orders_rounded = round(baseline_orders, 1)
        
        gemma_match = gemma_orders_rounded == claim["gemma_p1_orders"]
        baseline_match = baseline_orders_rounded == claim["baseline_prevalence_orders"]
        
        report.add(VerificationResult(
            claim_id="gemma_p1_and_baseline",
            claim_text=claim["claim_text"][:80] + "...",
            passed=gemma_match and baseline_match,
            expected=f"Gemma: {claim['gemma_p1_orders']} orders, Baseline: {claim['baseline_prevalence_orders']} orders",
            actual=f"Gemma: {gemma_orders_rounded} orders (raw: {gemma_p1_orders:.2f}), Baseline: {baseline_orders_rounded} orders",
            details=f"Baseline range: {claim['baseline_range_min_pct']}% to {claim['baseline_range_max_pct']}%. gemma_match={gemma_match}, baseline_match={baseline_match}",
            section=f"{section}/{subsection}"
        ))
    else:
        report.add(VerificationResult(
            claim_id="gemma_p1_and_baseline",
            claim_text=claim["claim_text"][:80] + "...",
            passed=False,
            expected="Gemma P1 data",
            actual="No Gemma family data found",
            details="Check model_family column",
            section=f"{section}/{subsection}"
        ))
    
    # Claim 3c_iv: P2 range
    # Use 2 significant figures matching
    claim = section_claims["p2_range"]
    p2_min, p2_max = min(p2_values), max(p2_values)
    p2_orders = np.log10(p2_max) - np.log10(p2_min)
    per_million_min = p2_min * 1e6
    per_million_max = p2_max * 1e6
    
    sig_figs = 2
    min_match = check_sig_figs_match(claim["min"], p2_min, sig_figs)
    max_match = check_sig_figs_match(claim["max"], p2_max, sig_figs)
    orders_match = round(p2_orders) == claim["orders_of_magnitude"]
    per_million_min_match = check_sig_figs_match(claim["per_million_min"], per_million_min, sig_figs)
    per_million_max_match = check_sig_figs_match(claim["per_million_max"], per_million_max, sig_figs)  # 2 sig figs for ~5100
    
    report.add(VerificationResult(
        claim_id="p2_range",
        claim_text=claim["claim_text"][:80] + "...",
        passed=min_match and max_match and orders_match and per_million_min_match and per_million_max_match,
        expected=f"{claim['min']:.1e} to {claim['max']:.1e}, {claim['orders_of_magnitude']} orders, {claim['per_million_min']} to {claim['per_million_max']} per million",
        actual=f"{format_sig_figs(p2_min, sig_figs)} to {format_sig_figs(p2_max, sig_figs)}, {round(p2_orders)} orders, {format_sig_figs(per_million_min, sig_figs)} to {format_sig_figs(per_million_max, sig_figs)} per million",
        details=f"At 1% baseline P(lack care -> harm). Using {sig_figs} sig figs. min_match={min_match}, max_match={max_match}, orders_match={orders_match}, per_million_min_match={per_million_min_match}, per_million_max_match={per_million_max_match}",
        section=f"{section}/{subsection}"
    ))
    
    # Claim 3c_v: P2 uncertainty pattern (similar to 3c_ii)
    claim = section_claims["p2_uncertainty_pattern"]
    
    # Check if CI columns exist (try both naming conventions)
    ci_lower_col = 'risk_ci_5' if 'risk_ci_5' in p2_data.columns else 'ci_lower'
    ci_upper_col = 'risk_ci_95' if 'risk_ci_95' in p2_data.columns else 'ci_upper'
    
    if ci_lower_col in p2_data.columns and ci_upper_col in p2_data.columns:
        p2_with_ci = p2_data.copy()
        p2_with_ci['ci_width'] = p2_with_ci[ci_upper_col] - p2_with_ci[ci_lower_col]
        p2_with_ci['ci_width_pct'] = p2_with_ci['ci_width'] / p2_with_ci['risk_probability'] * 100
        
        q25 = p2_with_ci['risk_probability'].quantile(0.25)
        q75 = p2_with_ci['risk_probability'].quantile(0.75)
        
        low_quartile = p2_with_ci[p2_with_ci['risk_probability'] <= q25]['ci_width_pct'].mean()
        high_quartile = p2_with_ci[p2_with_ci['risk_probability'] >= q75]['ci_width_pct'].mean()
        
        report.add(VerificationResult(
            claim_id="p2_uncertainty_pattern",
            claim_text=claim["claim_text"][:80] + "...",
            passed=low_quartile > high_quartile,
            expected="CI width % larger for low-P2 models than high-P2 models",
            actual=f"Low quartile: {low_quartile:.1f}% CI width, High quartile: {high_quartile:.1f}% CI width",
            details="Comparing CI width as % of point estimate",
            section=f"{section}/{subsection}"
        ))
    else:
        report.add(VerificationResult(
            claim_id="p2_uncertainty_pattern",
            claim_text=claim["claim_text"][:80] + "...",
            passed=False,
            expected="CI columns",
            actual="CI columns not found",
            details="Need risk_ci_5/risk_ci_95 or ci_lower/ci_upper columns",
            section=f"{section}/{subsection}"
        ))
    
    # Claim 3c_vi: CI overlap among high performers
    claim = section_claims["p2_ci_overlap"]
    
    # Check if CI columns exist (try both naming conventions)
    ci_lower_col = 'risk_ci_5' if 'risk_ci_5' in p2_data.columns else 'ci_lower'
    ci_upper_col = 'risk_ci_95' if 'risk_ci_95' in p2_data.columns else 'ci_upper'
    
    if ci_lower_col in p2_data.columns and ci_upper_col in p2_data.columns and 'model_family' in p2_data.columns:
        # Count overlapping CIs per family among models in top quartile
        q75 = p2_data['risk_probability'].quantile(0.75)
        top_performers = p2_data[p2_data['risk_probability'] <= q75]  # Lower P2 = better
        
        overlap_counts = {}
        for family in top_performers['model_family'].unique():
            family_models = top_performers[top_performers['model_family'] == family]
            if len(family_models) > 1:
                # Check for overlapping CIs
                overlaps = 0
                models_list = family_models.to_dict('records')
                for i, m1 in enumerate(models_list):
                    for m2 in models_list[i+1:]:
                        # CIs overlap if max(lower) < min(upper)
                        if max(m1[ci_lower_col], m2[ci_lower_col]) < min(m1[ci_upper_col], m2[ci_upper_col]):
                            overlaps += 1
                overlap_counts[family] = overlaps
        
        total_overlaps = sum(overlap_counts.values())
        
        report.add(VerificationResult(
            claim_id="p2_ci_overlap",
            claim_text=claim["claim_text"][:80] + "...",
            passed=total_overlaps > 0,
            expected="Several models with overlapping CIs",
            actual=f"Overlapping CI pairs per family: {overlap_counts}",
            details="Among top-performing (low P2) models",
            section=f"{section}/{subsection}"
        ))
    else:
        report.add(VerificationResult(
            claim_id="p2_ci_overlap",
            claim_text=claim["claim_text"][:80] + "...",
            passed=False,
            expected="CI and family columns",
            actual="Required columns not found",
            details="Need risk_ci_5/risk_ci_95 or ci_lower/ci_upper, plus model_family columns",
            section=f"{section}/{subsection}"
        ))


# =============================================================================
# SECTION 3d: CORRELATED FAILURE CLAIMS
# =============================================================================

def verify_correlated_failure_claims(report: VerificationReport, claims: dict, paper_run_dir: Path) -> None:
    """Verify correlated failure analysis claims."""
    
    section = "results"
    subsection = "correlated_failure"
    section_claims = claims[section][subsection]["claims"]
    
    # Claim 3d_ii: P2 independence range
    claim = section_claims["p2_independence_range"]
    p1p2_csv = paper_run_dir / "Data" / "processed_data" / "correlated_failure_analysis" / f"p1_p2_p_harm_values_m_{float(claim['m_value'])}.csv"
    
    if p1p2_csv.exists():
        p1p2_df = pd.read_csv(p1p2_csv)
        baseline_1pct = p1p2_df[p1p2_df['baseline_percentage'] == 1.0]
        p2_data = baseline_1pct[baseline_1pct['risk_type'] == 'P2']
        p2_values = [p for p in p2_data['risk_probability'].tolist() if p > 1e-15]
        
        if p2_values:
            p2_min, p2_max = min(p2_values), max(p2_values)
            p2_orders = np.log10(p2_max) - np.log10(p2_min)
            
            # Use 2 significant figures matching
            sig_figs = 2
            min_match = check_sig_figs_match(claim["min"], p2_min, sig_figs)
            max_match = check_sig_figs_match(claim["max"], p2_max, sig_figs)
            orders_match = round(p2_orders, 1) == claim["orders_of_magnitude"]
            
            report.add(VerificationResult(
                claim_id="p2_independence_range",
                claim_text=claim["claim_text"],
                passed=min_match and max_match and orders_match,
                expected=f"{claim['min']:.1e} to {claim['max']:.1e}, {claim['orders_of_magnitude']} orders",
                actual=f"{format_sig_figs(p2_min, sig_figs)} to {format_sig_figs(p2_max, sig_figs)}, {round(p2_orders, 1)} orders",
                details=f"Under independence (m={claim['m_value']}). Using {sig_figs} sig figs. min_match={min_match}, max_match={max_match}, orders_match={orders_match}",
                section=f"{section}/{subsection}"
            ))
        else:
            report.add(VerificationResult(
                claim_id="p2_independence_range",
                claim_text=claim["claim_text"],
                passed=False,
                expected="P2 values",
                actual="No valid P2 values found",
                details="All values were <= 1e-15",
                section=f"{section}/{subsection}"
            ))
    else:
        report.add(VerificationResult(
            claim_id="p2_independence_range",
            claim_text=claim["claim_text"],
            passed=False,
            expected="CSV file",
            actual="FILE NOT FOUND",
            details=f"Expected: {p1p2_csv}",
            section=f"{section}/{subsection}"
        ))
    
    # Claim 3d_iii: P2 convergence at high m
    claim = section_claims["p2_convergence"]
    high_m_csv = paper_run_dir / "Data" / "processed_data" / "correlated_failure_analysis" / f"p1_p2_p_harm_values_m_{float(claim['m_value'])}.csv"
    
    if high_m_csv.exists():
        high_m_df = pd.read_csv(high_m_csv)
        p2_high_m = high_m_df[(high_m_df['baseline_percentage'] == 1.0) & (high_m_df['risk_type'] == 'P2')]
        
        if len(p2_high_m) > 0:
            p2_mean = p2_high_m['risk_probability'].mean()
            p2_std = p2_high_m['risk_probability'].std()
            
            # Check convergence: mean should be close to 1e-2 and std should be small
            passed = abs(p2_mean - claim["convergence_value"]) < 0.002 and p2_std < 0.001
            
            report.add(VerificationResult(
                claim_id="p2_convergence",
                claim_text=claim["claim_text"][:80] + "...",
                passed=passed,
                expected=f"P2 converges to {claim['convergence_value']:.1e} (all models similar)",
                actual=f"P2 mean = {p2_mean:.4f}, std = {p2_std:.6f}",
                details=f"At m={claim['m_value']}, P2 should converge as engagement FNR -> 1",
                section=f"{section}/{subsection}"
            ))
        else:
            report.add(VerificationResult(
                claim_id="p2_convergence",
                claim_text=claim["claim_text"][:80] + "...",
                passed=False,
                expected="P2 data at baseline 1%",
                actual="No matching data found",
                details="Check baseline_percentage and risk_type filters",
                section=f"{section}/{subsection}"
            ))
    else:
        report.add(VerificationResult(
            claim_id="p2_convergence",
            claim_text=claim["claim_text"][:80] + "...",
            passed=False,
            expected="High-m CSV file",
            actual="FILE NOT FOUND",
            details=f"Expected: {high_m_csv}",
            section=f"{section}/{subsection}"
        ))


# =============================================================================
# REPORT GENERATION
# =============================================================================

def generate_report(report: VerificationReport, claims: dict, paper_run_dir: Path, cache_dir: Path) -> str:
    """Generate markdown verification report."""
    
    lines = [
        "# Manuscript Claims Verification Report",
        "",
        f"**Claims file:** `{CLAIMS_FILE}`",
        f"**Manuscript version:** {claims['_metadata']['manuscript_version']}",
        f"**Pipeline run:** `{paper_run_dir}`",
        f"**Cache directory:** `{cache_dir}`",
        "",
        "---",
        "",
    ]
    
    # Summary
    lines.extend([
        "## Summary",
        "",
        f"**Total claims verified:** {report.total_count}",
        f"**Passed:** {report.passed_count}",
        f"**Failed:** {report.failed_count}",
        "",
    ])
    
    if report.failed_count > 0:
        lines.append("### Failed Claims")
        lines.append("")
        for r in report.get_failed():
            lines.append(f"- **{r.claim_id}** ({r.section}): Expected `{r.expected}`, got `{r.actual}`")
        lines.append("")
    
    lines.append("---")
    lines.append("")
    
    # Abstract
    lines.extend([
        "## Abstract",
        "",
    ])
    for r in report.results:
        if r.section == "abstract":
            status = "PASSED" if r.passed else "FAILED"
            lines.append(f"### {r.claim_id}")
            lines.append(f"**Status:** {'✅' if r.passed else '❌'} {status}")
            lines.append(f"**Claim:** {r.claim_text}")
            lines.append(f"**Expected:** {r.expected}")
            lines.append(f"**Actual:** {r.actual}")
            if r.details:
                lines.append(f"**Details:** {r.details}")
            lines.append("")
    
    # Section 2: Methods
    lines.extend([
        "## Methods",
        "",
    ])
    for r in report.results:
        if r.section == "methods":
            status = "PASSED" if r.passed else "FAILED"
            lines.append(f"### {r.claim_id}")
            lines.append(f"**Status:** {'✅' if r.passed else '❌'} {status}")
            lines.append(f"**Claim:** {r.claim_text}")
            lines.append(f"**Expected:** {r.expected}")
            lines.append(f"**Actual:** {r.actual}")
            if r.details:
                lines.append(f"**Details:** {r.details}")
            lines.append("")
    
    # Section 3: Results
    lines.extend([
        "## Results",
        "",
    ])
    
    # Synthetic Data
    lines.append("### Synthetic Data")
    lines.append("")
    for r in report.results:
        if "synthetic_data" in r.section:
            status = "PASSED" if r.passed else "FAILED"
            lines.append(f"#### {r.claim_id}")
            lines.append(f"**Status:** {'✅' if r.passed else '❌'} {status}")
            lines.append(f"**Claim:** {r.claim_text}")
            lines.append(f"**Expected:** {r.expected}")
            lines.append(f"**Actual:** {r.actual}")
            if r.details:
                lines.append(f"**Details:** {r.details}")
            lines.append("")
    
    # Model Performance
    lines.append("### Model Performance")
    lines.append("")
    for r in report.results:
        if "model_performance" in r.section:
            status = "PASSED" if r.passed else "FAILED"
            lines.append(f"#### {r.claim_id}")
            lines.append(f"**Status:** {'✅' if r.passed else '❌'} {status}")
            lines.append(f"**Claim:** {r.claim_text}")
            lines.append(f"**Expected:** {r.expected}")
            lines.append(f"**Actual:** {r.actual}")
            if r.details:
                lines.append(f"**Details:** {r.details}")
            lines.append("")
    
    # P1/P2
    lines.append("### P1/P2 Risk Estimates")
    lines.append("")
    for r in report.results:
        if "p1_p2" in r.section:
            status = "PASSED" if r.passed else "FAILED"
            lines.append(f"#### {r.claim_id}")
            lines.append(f"**Status:** {'✅' if r.passed else '❌'} {status}")
            lines.append(f"**Claim:** {r.claim_text}")
            lines.append(f"**Expected:** {r.expected}")
            lines.append(f"**Actual:** {r.actual}")
            if r.details:
                lines.append(f"**Details:** {r.details}")
            lines.append("")
    
    # Correlated Failure
    lines.append("### Correlated Failure Analysis")
    lines.append("")
    for r in report.results:
        if "correlated_failure" in r.section:
            status = "PASSED" if r.passed else "FAILED"
            lines.append(f"#### {r.claim_id}")
            lines.append(f"**Status:** {'✅' if r.passed else '❌'} {status}")
            lines.append(f"**Claim:** {r.claim_text}")
            lines.append(f"**Expected:** {r.expected}")
            lines.append(f"**Actual:** {r.actual}")
            if r.details:
                lines.append(f"**Details:** {r.details}")
            lines.append("")
    
    lines.extend([
        "---",
        "",
        f"*Report generated by `analysis/manuscript_claims_verification.py`*",
    ])
    
    return "\n".join(lines)


# =============================================================================
# MAIN
# =============================================================================

def verify_claims(paper_run_dir: Path, cache_dir: Path) -> str:
    """Verify all manuscript claims against pipeline output."""
    
    claims = load_claims()
    report = VerificationReport()
    
    # Section 2: Methods - Cache parameters (query cache directly)
    verify_cache_parameters(cache_dir, report, claims)
    
    # Section 2: Methods - Config/code parameters
    verify_config_parameters(report, claims)
    
    # Section 1: Abstract claims
    verify_abstract_claims(report, claims, paper_run_dir)
    
    # Section 3a: Synthetic data claims
    verify_synthetic_data_claims(report, claims)
    
    # Section 3b: Model performance claims
    verify_model_performance_claims(report, claims, paper_run_dir)
    
    # Section 3c: P1/P2 claims
    verify_p1_p2_claims(report, claims, paper_run_dir)
    
    # Section 3d: Correlated failure claims
    verify_correlated_failure_claims(report, claims, paper_run_dir)
    
    return generate_report(report, claims, paper_run_dir, cache_dir)


def main():
    parser = argparse.ArgumentParser(description="Verify manuscript claims against pipeline output")
    parser.add_argument("--paper-run-dir", required=True, help="Path to pipeline output directory")
    parser.add_argument("--output", default="manuscript_claims_verification.md", help="Output filename")
    parser.add_argument("--cache-dir", default=str(DEFAULT_CACHE_DIR), help="Path to cache directory")
    parser.add_argument("--claims-file", default=None, help="Path to claims JSON (default: config/manuscript_claims.json)")
    args = parser.parse_args()
    
    global CLAIMS_FILE
    if args.claims_file:
        CLAIMS_FILE = Path(args.claims_file)
    
    paper_dir = Path(args.paper_run_dir)
    cache_dir = Path(args.cache_dir)
    output_path = paper_dir / args.output if paper_dir.is_dir() else Path(args.output)
    
    report = verify_claims(paper_dir, cache_dir)
    
    with open(output_path, 'w') as f:
        f.write(report)
    
    print(f"✓ Verification report: {output_path}")


if __name__ == "__main__":
    main()
