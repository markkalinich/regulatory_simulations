{
  "_metadata": {
    "description": "Centralized registry of manuscript claims organized by paper section",
    "last_updated": "2026-02-01",
    "manuscript_version": "Lancet_DH_Revisions",
    "verification_approach": {
      "api_params": "Query cache directly to verify temperature, max_tokens, top_p",
      "statistical_params": "Verify config/code values match claimed parameters",
      "computed_values": "Use freshly-generated pipeline CSVs",
      "qualitative_thresholds": {
        "nearly_all": ">=90%",
        "fewer_than_half": "<50%",
        "generally": ">=75%",
        "largely_accurate": ">=75%",
        "frequently": ">=50%"
      }
    }
  },

  "abstract": {
    "description": "Claims appearing in abstract",
    "claims": {
      "p1_range": {
        "claim_text": "Estimated P₁ values (hazard to hazardous situation) ranged from 1·1 × 10⁻⁸ to 1·6 × 10⁻⁴",
        "min": 1.1e-8,
        "max": 1.6e-4,
        "baseline_pct": 1.0,
        "verification": "computed_from_csv"
      },
      "p2_range": {
        "claim_text": "P₂ (hazardous situation to harm) from 4·9 × 10⁻⁵ to 5·1 × 10⁻³",
        "min": 4.9e-5,
        "max": 5.1e-3,
        "baseline_pct": 1.0,
        "verification": "computed_from_csv"
      },
      "orders_of_magnitude": {
        "claim_text": "This is a span of approximately 4 orders of magnitude",
        "value": 4,
        "context": "Largest span for either P1 or P2 (P1 spans ~4 orders)",
        "verification": "computed_from_csv"
      }
    }
  },

  "methods": {
    "description": "Methodological claims about parameters and statistical approaches",
    "claims": {
      "decoding_params": {
        "claim_text": "All models were run with fixed decoding parameters (temperature=0·0, max_tokens=256, top_p=1·0)",
        "temperature": 0.0,
        "max_tokens": 256,
        "top_p": 1.0,
        "verification": "query_cache_directly"
      },
      "confidence_intervals": {
        "claim_text": "Uncertainty in classification performance was quantified using 95% confidence intervals (CIs), selected according to metric structure: metrics expressible as binomial proportions—parsing success rate, sensitivity, specificity, and accuracy—exact two-sided Clopper–Pearson confidence intervals were computed. Confidence intervals for F1 scores were estimated using non-parametric bootstrap, resampling the observed classification outcomes with replacement and recomputing F1 for each of 5,000 bootstrap replicates.",
        "ci_level": 0.95,
        "ci_method_binomial": "clopper_pearson",
        "ci_method_f1": "bootstrap",
        "n_bootstrap_replicates": 5000,
        "verification": "verify_config_and_code",
        "code_locations": {
          "clopper_pearson": "utilities/statistics.py:clopper_pearson_ci",
          "bootstrap_f1": "utilities/statistics.py:bootstrap_f1_ci",
          "n_bootstrap_usage": "analysis/comparative_analysis/multi_experiment_plot_transposed_provenance.py:101"
        }
      },
      "beta_floor": {
        "claim_text": "For models achieving perfect sensitivity (zero false negatives), we used the median of the Beta(1, n) binomial distribution as a conservative floor for the true false-negative rate",
        "beta_alpha": 1,
        "lod_ci_level": 0.50,
        "verification": "verify_config_and_code",
        "config_location": "config/regulatory_paper_parameters.py:lod_ci_level"
      },
      "monte_carlo": {
        "claim_text": "Uncertainty was propagated through downstream risk calculations using Bayesian Monte Carlo sampling. FNRs were modeled as random variables with Beta posterior distributions and sampled using 50,000 draws per task",
        "n_mc_samples": 50000,
        "verification": "verify_config_and_code",
        "config_location": "config/regulatory_paper_parameters.py:n_mc_samples"
      }
    }
  },

  "results": {
    "description": "Results section claims",
    
    "synthetic_data": {
      "description": "Gemini-generated synthetic data (Figure 3)",
      "claims": {
        "non_si_retained": {
          "claim_text": "481/500 for Non-suicidal Statements retained; performance declined for sad, ambiguous, and angry tone",
          "numerator": 481,
          "denominator": 500,
          "verification": "computed_from_csv"
        },
        "si_retention": {
          "claim_text": "Suicidal-ideation (SI) content had significantly lower retention (315/500, p < 0·001)",
          "numerator": 315,
          "denominator": 500,
          "p_value_threshold": 0.001,
          "verification": "computed_from_csv"
        },
        "therapy_request_retention": {
          "claim_text": "For therapy-request detection, 768/800 non-therapeutic statements were retained, while explicit therapy requests performed significantly worse (340/400, p < 0·001)",
          "non_therapeutic": {
            "numerator": 768,
            "denominator": 800
          },
          "explicit_requests": {
            "numerator": 340,
            "denominator": 400
          },
          "p_value_threshold": 0.001,
          "verification": "computed_from_csv"
        },
        "therapy_engagement_retention": {
          "claim_text": "Nearly all non-therapeutic and ambiguous conversations were retained, but fewer than half of simulated therapeutic exchanges met criteria without modification (Figure 3C, Table S3, p < 0·001)",
          "non_therapeutic_ambiguous_threshold_pct": 90,
          "therapeutic_threshold_pct": 50,
          "p_value_threshold": 0.001,
          "verification": "computed_from_csv",
          "qualitative_operators": {
            "non_therapeutic_ambiguous": ">=",
            "therapeutic": "<"
          }
        },
        "cbt_dbt_accuracy": {
          "claim_text": "CBT/DBT-based dialogues were largely accurate, whereas those involving psychodynamic content, diagnostic clarification, or medication discussion frequently broke the requested therapeutic frame and required revision or removal",
          "cbt_dbt_threshold_pct": 75,
          "psychodynamic_diagnosis_medication_threshold_pct": 50,
          "verification": "computed_from_csv",
          "qualitative_operators": {
            "cbt_dbt": ">=",
            "psychodynamic_diagnosis_medication": "<"
          }
        }
      }
    },

    "model_performance": {
      "description": "Model performance characteristics",
      "claims": {
        "lower_param_struggled": {
          "claim_text": "Lower-parameter models within the Gemma and LLaMA families struggled to generate responses that conformed to the required output structure",
          "definition": "At least 2 of 3 model families have their smallest model with less than 75% parse success rate",
          "families_to_check": ["gemma", "llama", "qwen"],
          "parse_rate_threshold_pct": 75,
          "min_families_failing": 2,
          "verification": "computed_from_csv"
        },
        "performance_improved_with_size": {
          "claim_text": "Performance across all metrics (sensitivity, specificity, accuracy, and F1 score) generally improved with model size, though this relationship varied substantially by task and model family",
          "definition": "At least 75% of model families show positive correlation between size and metric",
          "threshold_pct": 75,
          "metrics": ["sensitivity", "specificity", "accuracy", "f1"],
          "verification": "computed_from_csv"
        },
        "si_difficult_statements": {
          "claim_text": "19 statements (4·2%) were missed by >50% of the 14 models: 7 active SI with plan and preparation, 4 active SI with plan but no intent, 3 passive SI, and 5 ambiguous emotional statements",
          "count": 19,
          "percentage": 4.2,
          "total_models": 14,
          "threshold_miss_rate": 0.5,
          "breakdown": {
            "active_si_plan_with_intent_prep": 7,
            "active_si_plan_no_intent": 4,
            "passive_si": 3,
            "ambiguous": 5
          },
          "verification": "computed_from_csv"
        },
        "tr_difficult_statements": {
          "claim_text": "For therapy request classification, 4 statements (0·5%) were missed by >50% of models: 3 declarative statements with sad affect and 1 non-therapeutic question with sad affect",
          "count": 4,
          "percentage": 0.5,
          "threshold_miss_rate": 0.5,
          "breakdown": {
            "declarative_sad": 3,
            "non_therapeutic_question_sad": 1
          },
          "verification": "computed_from_csv"
        },
        "te_difficult_conversations": {
          "claim_text": "For therapy engagement classification, 7 conversations (1·7%) were missed by >50% of models: 4 clear engagement and 3 ambiguous engagement",
          "count": 7,
          "percentage": 1.7,
          "threshold_miss_rate": 0.5,
          "breakdown": {
            "clear_engagement": 4,
            "ambiguous_engagement": 3
          },
          "verification": "computed_from_csv"
        }
      }
    },

    "p1_p2": {
      "description": "P1 and P2 risk estimates (Figure 5, S11)",
      "claims": {
        "p1_range": {
          "claim_text": "Estimates of P1 ranged from 1·1 × 10⁻⁸ to 1·6 × 10⁻⁴ at a 1% baseline prevalence of suicidal ideation, spanning four orders of magnitude and corresponding to 0·011 to 161 hazardous situations per million individual user–model exchanges",
          "min": 1.1e-8,
          "max": 1.6e-4,
          "baseline_pct": 1.0,
          "orders_of_magnitude": 4,
          "per_million_min": 0.011,
          "per_million_max": 161,
          "verification": "computed_from_csv"
        },
        "p1_uncertainty_pattern": {
          "claim_text": "Uncertainty increased as estimated risk decreased: models with the lowest P1 point estimates exhibited uncertainty intervals large relative to the magnitude of those estimates",
          "definition": "For models in lowest P1 quartile, CI width as % of point estimate is larger than for models in highest quartile",
          "verification": "computed_from_csv"
        },
        "gemma_p1_and_baseline": {
          "claim_text": "Within the Gemma family alone, P1 spanned 3·3 orders of magnitude, while varying baseline suicidal ideation prevalence from 0·1% to 10% contributed an additional two orders of magnitude through linear scaling",
          "gemma_p1_orders": 3.3,
          "baseline_prevalence_orders": 2,
          "baseline_range_min_pct": 0.1,
          "baseline_range_max_pct": 10,
          "verification": "computed_from_csv"
        },
        "p2_range": {
          "claim_text": "Estimates of P2 ranged from 4·9 × 10⁻⁵ to 5·1 × 10⁻³ at a 1% baseline probability that lack of professional care results in harm, spanning approximately two orders of magnitude and corresponding to 49 to ~5,100 harms per million individual user–model four-turn exchanges",
          "min": 4.9e-5,
          "max": 5.1e-3,
          "baseline_pct": 1.0,
          "orders_of_magnitude": 2,
          "per_million_min": 49,
          "per_million_max": 5100,
          "verification": "computed_from_csv"
        },
        "p2_uncertainty_pattern": {
          "claim_text": "As with P1, uncertainty was greatest for models with the lowest estimated risk, reflecting limited statistical resolution when characterizing rare downstream failures from modestly sized evaluation sets",
          "definition": "For models in lowest P2 quartile, CI width as % of point estimate is larger than for models in highest quartile",
          "verification": "computed_from_csv"
        },
        "p2_ci_overlap": {
          "claim_text": "Uncertainty intervals overlapped across several high-performing models, limiting the ability to distinguish their relative downstream risk contributions based on point estimates alone",
          "definition": "Count models per family with overlapping CIs among top performers",
          "verification": "computed_from_csv"
        }
      }
    },

    "correlated_failure": {
      "description": "Correlated failure analysis",
      "claims": {
        "failure_multiplier_range": {
          "claim_text": "To examine the impact of correlated detection failures, we varied the failure-multiplier parameter from independence (m = 1) to strong conditional dependence (m = 100,000)",
          "m_min": 1,
          "m_max": 100000,
          "verification": "verify_config_and_code",
          "config_location": "config/regulatory_paper_parameters.py:failure_multiplier_values"
        },
        "p2_independence_range": {
          "claim_text": "Under independence, P2 ranged from 4·9 × 10⁻⁵ to 5·1 × 10⁻³ across models (as above), spanning 2·0 orders of magnitude",
          "min": 4.9e-5,
          "max": 5.1e-3,
          "m_value": 1,
          "orders_of_magnitude": 2.0,
          "verification": "computed_from_csv"
        },
        "p2_convergence": {
          "claim_text": "Under strong dependence, the adjusted therapy-engagement false-negative term approaches 1 for all models, causing P2 to converge to 1·0 x 10⁻²—that is, P(fail to seek help) x P(lack of care → harm)—and rendering engagement-detection performance effectively irrelevant, collapsing all model trajectories onto a single overlapping curve",
          "convergence_value": 1.0e-2,
          "m_value": 100000,
          "verification": "computed_from_csv"
        }
      }
    }
  }
}
